{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fbcc1f1",
   "metadata": {},
   "source": [
    "For a discussion of what is text and data mining? Refer to this [introductory session](https://github.com/SouthernMethodistUniversity/introTDM/blob/main/introTDM.md)\n",
    "\n",
    "# How do you find data for TDM projects ?\n",
    "\n",
    "## Glossary\n",
    " > \"What are the differences between data, a dataset, and a database?\n",
    "  * *Data* are observations or measurements (unprocessed or processed) represented as text, numbers, or multimedia.\n",
    "  * A *dataset* is a structured collection of data generally associated with a unique body of work.\n",
    "  * A *database* is an organized collection of data stored as multiple datasets. Those datasets are generally stored and accessed electronically from a computer system that allows the data to be easily accessed, manipulated, and updated. \n",
    "  - [Definition via USGS](https://www.usgs.gov/faqs/what-are-differences-between-data-dataset-and-database#:~:text=Data%20are%20observations%20or%20measurements,a%20unique%20body%20of%20work.) \n",
    "* For the purposes of this workshop series, we recommend finding an already existing data set for your project, as  creating, cleaning and/or structuring a new dataset is often time and labor intensive. \n",
    "* Read more about [Data Prep and Cleaning](https://digitalhumanities.berkeley.edu/data-prep-and-cleaning) and [Cleaning Text Data](https://medhieval.com/classes/hh2019/labs/cleaning-text-data/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df417224",
   "metadata": {},
   "source": [
    "# Text as Data \n",
    "\n",
    "When approaching text as data, here are some things to keep in mind:\n",
    "\n",
    "* First, having textual data of sufficient quality is important. Textual data quality is determined by how it’s created. Hand-keyed text is often of the best quality, while text obtained by OCR, Optical Character Recognition, can vary in quality. Raw, uncorrected OCR text is dirty, and it can only become clean until it is corrected. (For examale, Please note that HathiTrust OCR is dirty and uncorrected.\n",
    "* When viewing text as data, we usually analyze them by corpus or corpora. As mentioned in previous modules, a “corpus” of text can refer to both a digital collection and an individual's research text dataset. Text corpora are bodies of text.\n",
    "* When preparing text, one can think in terms of what [Geoffrey Rockwell has called text decomposition or re-composition.](https://geoffreyrockwell.com/publications/WhatIsTAnalysis.pdf) The text will be split, combined, or represented in ways that distinguish it from human readable text. It may involve discarding some text, and requires the researcher to shift their understanding of the text from human-legible object to data. What stays, what goes, and how things are manipulated is a researcher’s choice. While there are emerging best practices, there isn’t a step-by-step guide to follow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e555a1ad",
   "metadata": {},
   "source": [
    "## What makes a useful dataset?\n",
    "\n",
    "For the purposes of completing a project in a semester, we recommend choosing an already existing 'good' data set so that you can start [your analysis.](https://www.usgs.gov/data-management/analyze)\n",
    "\n",
    "* What do we mean by a 'good' data set? \n",
    "\n",
    "The considerations you should keep in mind for [creating a data set,](https://researchdata.ox.ac.uk/home/managing-your-data-at-oxford/organising-your-data/) are the same things you want to look for when searching for a dataset for research. \n",
    "    \n",
    "   * Is there a [Data dictionary](https://www.usgs.gov/data-management/data-dictionaries) or any kind of documentation that states how the date was derived and why it was recording in the way it was. \n",
    "    \n",
    "   * Good documentation makes material *understandable, verifiable, and reusable* (by you or by others).\n",
    "    \n",
    "   * What are the file naming conventions? \n",
    "    \n",
    "   * Is there metadata ?\n",
    "       * Metadata is simply ‘data about data’.  It is related to the broader contextual information that describes your data, but is usually more structured in that it conforms to set standards and is machine readable.  One typical use of metadata is to create a catalogue record for a dataset held in an archive. By using a standard set of tags, an automatic system can tell where the information about the title, creator, description and so forth begin and end.\n",
    "    \n",
    "   * Data that is not structured or cleaned is referred to as unstructured, noisy or dirty. *A messy data set can be used but you will need to spend time [processing that data:](https://www.usgs.gov/data-management/process) either cleaning, structuring and/or organizing it.* \n",
    "\n",
    "Read more: \n",
    "* [DH@ Berkley:Data Prep and Cleaning](https://digitalhumanities.berkeley.edu/data-prep-and-cleaning)\n",
    "* \"Data cleaning is the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset. When combining multiple data sources, there are many opportunities for data to be duplicated or mislabeled.\" \n",
    "    * How do you clean data?\n",
    "        * Step 1: Remove duplicate or irrelevant observations\n",
    "        * Step 2: Fix structural errors\n",
    "        * Step 3: Filter unwanted outliers\n",
    "        * Step 4: Handle missing data\n",
    "        * Step 5: Validate and QA (quality assurance)\n",
    "            * [Guide To Data Cleaning: Definition, Benefits, Components, And How To Clean Your Data](https://www.tableau.com/learn/articles/what-is-data-cleaning) \n",
    "\n",
    "# What are the stages of data ?\n",
    "* We begin without data. Then it is *observed*, or *made*, or *imagined*, or *generated.* After that, it goes through further transformations.\n",
    "\n",
    "## Raw data \n",
    "* Raw data is yet to be processed, meaning it has yet to be manipulated by a human or computer. Received or collected data could be in any number of formats, locations, etc.. It could be in any number of forms.\n",
    "* But \"raw data\" is a relative term, inasmuch as when one person finishes processing data and presents it as a finished product, another person may take that product and work on it further, and for them that data is \"raw data\". \n",
    "    * For example, is \"big data\" \"raw data\"? How do we understand data that we have \"scraped\"?\n",
    "\n",
    "## Processed/transformed\n",
    "\n",
    "* Processing data puts it into a state more readily available for analysis, and makes the data legible. For instance it could be rendered as **structured data**. This can also take many forms, e.g., a table. \n",
    "\n",
    "* Here are a few you're likely to come across, all representing the same data:\n",
    "\n",
    "### XML\n",
    "\n",
    "```\n",
    "<Cats> \n",
    "    <Cat> \n",
    "        <firstName>Smally</firstName> <lastName>McTiny</lastName> \n",
    "    </Cat> \n",
    "    <Cat> \n",
    "        <firstName>Kitty</firstName> <lastName>Kitty</lastName> \n",
    "    </Cat> \n",
    "    <Cat> \n",
    "        <firstName>Foots</firstName> <lastName>Smith</lastName> \n",
    "    </Cat> \n",
    "    <Cat> \n",
    "        <firstName>Tiger</firstName> <lastName>Jaws</lastName> \n",
    "    </Cat> \n",
    "</Cats> \n",
    "```\n",
    "\n",
    "### JSON\n",
    "\n",
    "```\n",
    "{\"Cats\":[ \n",
    "    { \"firstName\":\"Smally\", \"lastName\":\"McTiny\" }, \n",
    "    { \"firstName\":\"Kitty\", \"lastName\":\"Kitty\" }, \n",
    "    { \"firstName\":\"Foots\", \"lastName\":\"Smith\" }, \n",
    "    { \"firstName\":\"Tiger\", \"lastName\":\"Jaws\" } \n",
    "]} \n",
    "```\n",
    "\n",
    "### CSV\n",
    "```\n",
    "First Name,Last Name/n\n",
    "Smally,McTiny/n\n",
    "Kitty,Kitty/n\n",
    "Foots,Smith/n\n",
    "Tiger,Jaws/n\n",
    "```\n",
    "\n",
    "### The importance of using open data formats\n",
    "A small detour to discuss (the ethics of?) data formats. For accessibility, future-proofing, and preservation, keep your data in open, sustainable formats. A demonstration:\n",
    "\n",
    "Sustainable formats are generally unencrypted, uncompressed, and follow an open standard. A small list:\n",
    "\n",
    "* ASCII\n",
    "* PDF \n",
    "* .csv\n",
    "* FLAC\n",
    "* TIFF\n",
    "* JPEG2000\n",
    "* MPEG-4\n",
    "* XML\n",
    "* RDF\n",
    "* .txt\n",
    "* .r\n",
    "\n",
    "How do you decide the formats to store your data when you transition from 'raw' to 'processed/transformed' data? What are some of your considerations?\n",
    "\n",
    "## Tidy data\n",
    "There are guidelines to the processing of data, sometimes referred to as **Tidy Data**.<sup>1</sup> One manifestation of these rules:\n",
    "1. Each variable is in a column.\n",
    "2. Each observation is a row.\n",
    "3. Each value is a cell.\n",
    "\n",
    "Look back at our example of cats to see how they may or may not follow those guidelines. **Important note**: Some data formats allow for more than one dimension of data! How might that complicate the concept of **Tidy Data**?\n",
    "\n",
    "```\n",
    "{\"Cats\":[\n",
    "    {\"Calico\":[\n",
    "    { \"firstName\":\"Smally\", \"lastName\":\"McTiny\" },\n",
    "    { \"firstName\":\"Kitty\", \"lastName\":\"Kitty\" }],\n",
    "    \"Tortoiseshell\":[\n",
    "    { \"firstName\":\"Foots\", \"lastName\":\"Smith\" }, \n",
    "    { \"firstName\":\"Tiger\", \"lastName\":\"Jaws\" }]}]}\n",
    "```\n",
    "\n",
    " <sup>1</sup>Wickham, Hadley. \"Tidy Data\". Journal of Statistical Software.\n",
    "\n",
    "## Forms of data\n",
    "\n",
    "There are many ways to represent data, just as there are many sources of data. For the purposes of this series we are focusing on already digitized text. In the next session workshop in this series, you will see how a .csv file can be processed an analysed using Python. \n",
    "\n",
    "# You can search for already existing datasets in the following:\n",
    "\n",
    "**Databases**\n",
    "*Licensed content in Library databases*\n",
    "Some databases allow for text mining. Those are [marked on our A-Z list with ](https://guides.smu.edu/az.php?t=45104) *\n",
    "\n",
    "*Policies for Mining Licensed Content* \n",
    "If you are thinking of basing a research project on data extracted from a library database, contact your [subject librarian](https://www.smu.edu/libraries/help/librarian) to discuss issues around permissions (copyright and licensing agreements), formats and fees.\n",
    "\n",
    "In addition to [copyright](https://www.smu.edu/Libraries/scholarship/copyright) considerations, we must take into account what the database vendors’ own policies specify in regard to this type of use. When providing access to a database, the library enters into licensing agreements, which also dictate what types of data can be extracted and used. Many prohibit text and data mining and the use of software such as scripts, agents, or robots, but it may be possible to negotiate text mining rights.\n",
    "\n",
    "*Licensed content in Library databases*\n",
    "Some databases allow for text mining. Those are [marked on our A-Z list with the filter Text Mining](https://guides.smu.edu/az.php?t=45104) \n",
    "* [HathiTrust Research Center for TDM](https://github.com/SouthernMethodistUniversity/introTDM/blob/main/Sections/HTRC.md)\n",
    "* [JSTOR Constellate for TDM](https://github.com/SouthernMethodistUniversity/introTDM/blob/main/Sections/Constellate.md)\n",
    "\n",
    "*Policies for Mining Licensed Content* \n",
    "If you are thinking of basing a research project on data extracted from a library database, contact your [subject librarian](https://www.smu.edu/libraries/help/librarian) to discuss issues around permissions (copyright and licensing agreements), formats and fees.\n",
    "\n",
    "In addition to [copyright](https://www.smu.edu/Libraries/scholarship/copyright) considerations, we must take into account what the database vendors’ own policies specify in regard to this type of use. When providing access to a database, the library enters into licensing agreements, which also dictate what types of data can be extracted and used. Many prohibit text and data mining and the use of software such as scripts, agents, or robots, but it may be possible to negotiate text mining rights.\n",
    "\n",
    "*Non-consumptive or non-expressive use*\n",
    "\n",
    "* Research in which computational analysis is performed on text, but not research in which a researcher reads or displays substantial portions of the text to understand the expressive content presented within it.\n",
    "* Non-consumptive research complies with copyright law because of the distinction in law between “ideas” and “expressions”. It is sometimes called non-expressive use (because it works with “ideas” instead of specific “expressions”, hence the term “non-expressive”). \n",
    "    * Non-consumptive research complies with copyright law because of the distinction in law between “ideas” and “expressions”. It is sometimes called non-expressive use (because it works with “ideas” instead of specific “expressions”, hence the term “non-expressive”). \n",
    "* Foundation of [HTRC](https://github.com/SouthernMethodistUniversity/introTDM/blob/main/Sections/HTRC.md) work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513d6409",
   "metadata": {},
   "source": [
    "**Open access (OA) or Public Domain information**\n",
    "\n",
    "You can [search](https://guides.smu.edu/internetsearching) the open web using a web browser such as Chrome or Firefox, adding specific terms such as: data, datasets, API, file format (such as .csv). [Advanced search](https://www.google.com/advanced_search) options amy also allow for searching for specific file types. \n",
    "Try searching: \n",
    "* [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page)\n",
    "* [Wikipedia](https://en.wikipedia.org/wiki/Main_Page)\n",
    "* [Wikisource](https://en.wikisource.org/wiki/Main_Page)\n",
    "* [Library of Congress Labs](https://labs.loc.gov/)\n",
    "* [Digital Public Library](https://dp.la/)\n",
    "* [Project Gutenberg](https://www.gutenberg.org/)\n",
    "* You can also look at the archive for [*Data Is Plural* a weekly newsletter of datasets](https://www.data-is-plural.com/)\n",
    "\n",
    "Depending on the the type of data, the collecting agency or you field their might be open access repositories with that data available.\n",
    "* Data collected by the [U.S. government may be publicly available.](https://usafacts.org/data/)\n",
    "* Your discipline may have a OA repository, such as [arXiv, which has articles in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics.](https://arxiv.org/)\n",
    "\n",
    "**Data and methodology sections of research and data journalism articles**\n",
    "\n",
    "When reading articles on topics of interest, always pay attention to the *data and methodology sections.* \n",
    "* Where did these researchers find their data set they are using? \n",
    "* If they created it, did they make it accessible in a repository on a website, or a Github repository?\n",
    "\n",
    "Read some data journalism articles on [The Pudding](https://pudding.cool/), [ProPublica](https://www.propublica.org/datastore) or this [roundup of data journalism projects from 2021](https://datajournalism.com/read/blog/best-data-journalism-projects-2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a829f32",
   "metadata": {},
   "source": [
    "# Preparing Data \n",
    "* After gathering the data needed for research and before conducting the actual analysis, data often requires preparation. Preparing data can take a lot of time and effort.\n",
    "* Examples of what may be necessary to do before the data is in a workable state: \n",
    "    * Correcting OCR errors.\n",
    "    * Removing title and header information.\n",
    "    * Removing html or xml tags.\n",
    "    * Splitting or combining files. \n",
    "    * Removing certain words or punctuation marks.\n",
    "    * Making text into lowercase.\n",
    "    \n",
    "[![Chunking text](https://github.com/SouthernMethodistUniversity/introTDM/blob/main/images/chunktext.png)](https://github.com/SouthernMethodistUniversity/introTDM/blob/main/images/chunktext.png)\n",
    "* As mentioned, preparing text often involves splitting and combining files. In text analysis, splitting files is commonly referred to as chunking text. It means splitting text into smaller pieces before analysis. The text may be divided by paragraph, chapter, or a chosen number of words (e.g. 1000 word chunks). Let’s say that we have a whole text that consist of speeches of Abraham Lincoln. Before conducting analysis, the researcher may need to split the text into individual speeches. This process can be called chunking text.\n",
    "\n",
    "[![Grouping text](https://github.com/SouthernMethodistUniversity/introTDM/blob/main/images/groupingtext.png)](https://github.com/SouthernMethodistUniversity/introTDM/blob/main/images/groupingtext.png)\n",
    "* An opposite process that needs to be done just as often is combining text into larger pieces before analysis, which can be referred to as grouping text. Let’s look at political speeches as an example. Say that this time we have individual texts of various speeches made by Abraham Lincoln as well as George Washington. Before conducting our analysis, we may need to group the texts by combining all speeches by Lincoln into one group and all speeches by Washington into another group. \n",
    "\n",
    "Both chunking (from the previous image) and grouping are ways of modifying the unit of analysis for the researcher, and it’s wholly dependent on what the researcher wants to study. Maybe someone wants to compare all of Abraham Lincoln to all of George Washington, then they could create two large “buckets” of data via chunking. Or someone only wants to compare the chapters in John F. Kennedy’s “Profiles in Courage” to see how descriptions of the figures it profiled are similar or different, then a researcher might split a single work out by chapter. Those are simplistic examples, but they highlight the kinds of splitting and combining that may happen. \n",
    "\n",
    "[![Tokenizations](https://github.com/SouthernMethodistUniversity/introTDM/blob/main/images/token.png)](https://github.com/SouthernMethodistUniversity/introTDM/blob/main/images/token.png)\n",
    "* An additional step in preparation is called tokenization. Tokenization is simply the process of breaking text into pieces called tokens. Often certain characters, such as punctuation marks, are discarded in the process. \n",
    "Here’s a tokenized version of the beginning of The Gettysburg Address on the slide. The original text, which is in a human-readable form, has been translated into tokens. \n",
    "While the tokens can still be parsed by a human, it isn’t in a form we regularly read. It can now, however, be read and processed by a computer. \n",
    "\n",
    "* It is important to note that *different choices in text preparation will affect the results of the analysis.* \n",
    "* Depending on the amount of text and size of chunks, which stop words are removed and which characters are included, and whether to lowercase and normalize words, the eventual text that is ready for analysis can be very different. Additionally, preparation for analysis takes a lot of time and effort. This is where scripting becomes useful!\n",
    "\n",
    "* [Additional information about how text preparation impacts results](https://ssrn.com/abstract=2849145)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271bf250",
   "metadata": {},
   "source": [
    "# Approaches to TDM\n",
    "\n",
    "### What emotions are expressed?\n",
    "[![Natural Language Processing (NLP)](https://github.com/SouthernMethodistUniversity/introTDM/blob/main/images/nlp.png)](https://github.com/SouthernMethodistUniversity/introTDM/blob/main/images/nlp.png)\n",
    "\n",
    "* One key approach is *Natural Language Processing (NLP)*, meaning using computers to understand the meaning, relationships, and semantics within human-language text. Generally for natural language processing, full text is needed. It is not a bag-of-words method. Some common, specific methods under NLP are:\n",
    "    \n",
    "    * Named entity extraction, which uses computers to learn about what names of people, places, and organizations are in the text.\n",
    "    * Sentiment analysis, which uses computers to explore what emotions are present in the text. \n",
    "    * Stylometry, which uses computers to speculate who wrote the text based on language style.\n",
    "\n",
    "### What patterns are present? \n",
    "\n",
    "[![Machine Learning (ML)](https://github.com/SouthernMethodistUniversity/introTDM/blob/main/images/ml.png)](https://github.com/SouthernMethodistUniversity/introTDM/blob/main/images/ml.png)\n",
    "* Another key approach to text analysis is *Machine Learning (ML)*, which is training computers to recognize patterns in text without explicit human programming. Machine learning can either be unsupervised (with minimal human intervention) or supervised (with more human intervention). Here are some common, specific methods that are based on machine learning:  \n",
    "\n",
    "    * Topic modeling, which explores the thematic topics present in the text. Remember that topic modeling is a bag-of-words approach. \n",
    "    * Naïve Bayes classification, which explores the categorization of texts, i.e. determining what categories that the researcher have named does a certain text belong to. \n",
    "\n",
    "# Topic modeling\n",
    "* Topic modeling is a method of using statistical models for discovering the abstract \"topics\" that occur in a collection of documents.\n",
    "* This image visualizes what happens in a topic model. \n",
    "[![Topic Modeling](https://github.com/SouthernMethodistUniversity/introTDM/blob/main/images/topicmodel.png)](https://github.com/SouthernMethodistUniversity/introTDM/blob/main/images/topicmodel.png)\n",
    "* For this kind of analysis, the text is chunked, and stop words (frequently used words such as “the”, “and”, “if”) are removed since they reveal little about the substance of a text. \n",
    "* The computer treats the textual documents as bags of words, and guesses which words make up a “topic” based on their proximity to one another in the documents, with the idea the words that frequently co-occur are likely about the same thing. So the different colored groupings are the groups of words that the computer has statistically analyzed and determined are likely related to each other about a “topic”.\n",
    "\n",
    "## Bag-of-words\n",
    "* “Bag-of-words” is a concept where grammar and word order of the original text are disregarded and frequency is maintained. Here is an example of the beginning of The Gettysburg Address as a bag of words.\n",
    "[![Bag Of Words](https://github.com/SouthernMethodistUniversity/introTDM/blob/main/images/bagwords.png)](https://github.com/SouthernMethodistUniversity/introTDM/blob/main/images/bagwords.png)\n",
    "\n",
    "* Here are some tips for topic modeling:\n",
    " * Treat topic modeling as one part of a larger analysis.\n",
    "* Understand what you input, including how you set your parameters, will affect the output. Some points to note are:\n",
    "* Be careful with how you set the number of texts analyzed, as well as number of topics generated\n",
    "* Be familiar with your input data\n",
    "* Know that changing your stop word list can have really interesting impacts on your topics, so tread carefully/wisely.\n",
    "* You’re going to want to go back to the text at some point. Make sure to examine your results to see if they make sense.\n",
    "* Also, try to gain some basic understanding of your tool. *Reading some relevant documentation is especially important when the tool is within a “black box”.*\n",
    "    * For example the HTRC algorithm only has a few parameters you can set, so it’s not suitable for really robust topic modeling. But for teaching and exploration of HT text specifically, [the HTRC topic modeling algorithm can be a good place to start!](https://analytics.hathitrust.org/algorithms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa401773",
   "metadata": {},
   "source": [
    "# Next Session\n",
    "* In the next workshop we will be introducing Python. \n",
    "* Python is a commonly-used programming language, and it’s very useful for working with data. It is also an [interpreted language, which basically means it follows step-by-step directions.](https://www.freecodecamp.org/news/compiled-versus-interpreted-languages/) \n",
    "* Additionally, Python is generally easy to learn with its relatively simple syntax. For example, one of its learner-friendly features is it avoids excess punctuation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e9d92",
   "metadata": {},
   "source": [
    "Read more about [Data in digital humanities](https://github.com/SouthernMethodistUniversity/data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4fa7f9",
   "metadata": {},
   "source": [
    "Some content on this page adapted from:\n",
    "    \n",
    "* [Research Data Oxford](https://researchdata.ox.ac.uk/home/managing-your-data-at-oxford/organising-your-data/) and used under a [Creative Commons Attribution 3.0 Unported License.](https://creativecommons.org/licenses/by/3.0/).\n",
    "* [BOSTON COLLEGE LIBRARIES:Text & Data Mining](https://libguides.bc.edu/textdatamining/overview) and used under a [Creative Commons Attribution 4.0 International License..](https://creativecommons.org/licenses/by/4.0/).\n",
    "* [NYU Text Data Mining](https://guides.nyu.edu/tdm/start)and used under a [ Creative Commons Attribution-NonCommercial 4.0 International License.](https://creativecommons.org/licenses/by-nc/4.0/).\n",
    "* [HTRC Digging Deeper, Reaching Further](https://teach.htrc.illinois.edu/teaching-materials/) used under a [Creative Commons Attribution-NonCommercial 4.0 International License.](https://creativecommons.org/licenses/by-nc/4.0/)\n",
    "* Also see:[USGS data management](https://www.usgs.gov/data-management) "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
